# DualT-Fusion
Multimodal data sources are becoming increasingly important across a wide range of scientific and real-world domains, including astrophysics, remote sensing, and healthcare, where complementary information is often captured by heterogeneous signals such as multivariate time series and images. Effectively integrating these modalities remains nontrivial due to differences in structure, scale, and information density, as well as limited labeled data. We propose DualT-Fusion, a unified transformer-based framework that jointly models temporal dynamics and visual patterns while enabling robust cross-modal alignment. The framework incorporates a Transformer encoder designed for multivariate time series, alongside a Vision Transformer for image data. To facilitate effective cross-modal interaction, we introduce a lightweight cross-attention fusion mechanism with a learnable temperature parameter that adaptively regulates the strength of cross-modal integration. In addition, DualT-Fusion employs a supervised contrastive objective to enforce class-consistent alignment between modality-specific embeddings while preserving modality-specific information. The resulting fused representation is used for downstream classification via a simple linear head, with the overall objective automatically balancing supervised classification and contrastive alignment. Experiments on multimodal solar flare benchmark dataset demonstrate that DualT-Fusion shows effectiveness for learning coherent representations from heterogeneous scientific data.

# Includes main framework module and dataset creation scripts
